{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9b37a14",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>attributes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00005LE4P</td>\n",
       "      <td>Laptops</td>\n",
       "      <td>ThinkPad T22 2647 - PIII 900 MHz - RAM 128 MB ...</td>\n",
       "      <td>[{'key': 'Screen Size', 'evidences': [{'value'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00005LE4P</td>\n",
       "      <td>Laptops</td>\n",
       "      <td>ThinkPad T22 2647 - PIII 900 MHz - RAM 128 MB ...</td>\n",
       "      <td>[{'key': 'Processor Speed', 'evidences': [{'va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00005LE4P</td>\n",
       "      <td>Laptops</td>\n",
       "      <td>ThinkPad T22 2647 - PIII 900 MHz - RAM 128 MB ...</td>\n",
       "      <td>[{'key': 'Resolution', 'evidences': [{'value':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00005NBJB</td>\n",
       "      <td>Laptops</td>\n",
       "      <td>Apple iBook Laptop (500-MHz PowerPC G3, 128 MB...</td>\n",
       "      <td>[{'key': 'Processor Speed', 'evidences': [{'va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00005NBIS</td>\n",
       "      <td>Laptops</td>\n",
       "      <td>Apple iBook Laptop (500-MHz PowerPC G3, 64 MB ...</td>\n",
       "      <td>[{'key': 'Processor Speed', 'evidences': [{'va...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id category                                              title  \\\n",
       "0  B00005LE4P  Laptops  ThinkPad T22 2647 - PIII 900 MHz - RAM 128 MB ...   \n",
       "1  B00005LE4P  Laptops  ThinkPad T22 2647 - PIII 900 MHz - RAM 128 MB ...   \n",
       "2  B00005LE4P  Laptops  ThinkPad T22 2647 - PIII 900 MHz - RAM 128 MB ...   \n",
       "3  B00005NBJB  Laptops  Apple iBook Laptop (500-MHz PowerPC G3, 128 MB...   \n",
       "4  B00005NBIS  Laptops  Apple iBook Laptop (500-MHz PowerPC G3, 64 MB ...   \n",
       "\n",
       "                                          attributes  \n",
       "0  [{'key': 'Screen Size', 'evidences': [{'value'...  \n",
       "1  [{'key': 'Processor Speed', 'evidences': [{'va...  \n",
       "2  [{'key': 'Resolution', 'evidences': [{'value':...  \n",
       "3  [{'key': 'Processor Speed', 'evidences': [{'va...  \n",
       "4  [{'key': 'Processor Speed', 'evidences': [{'va...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load datasets with pandas\n",
    "import pandas as pd\n",
    "\n",
    "_DATASET_DIR = 'C:/Users/ADMIN/Desktop/DATN/Extract_information/data/split_mave/'\n",
    "\n",
    "df_train_positives = pd.read_json(_DATASET_DIR + 'train/mave_positives.jsonl', lines=True)\n",
    "df_train_negatives = pd.read_json(_DATASET_DIR + 'train/mave_negatives.jsonl', lines=True)\n",
    "df_train = pd.concat([df_train_positives, df_train_negatives])\n",
    "\n",
    "df_val_positives = pd.read_json(_DATASET_DIR + 'eval/mave_positives.jsonl', lines=True)\n",
    "df_val_negatives = pd.read_json(_DATASET_DIR + 'eval/mave_negatives.jsonl', lines=True)\n",
    "df_val = pd.concat([df_val_positives, df_val_negatives])\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6f7070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by product id\n",
    "df_train_grouped = df_train.groupby(['id']).agg({'attributes': 'sum', 'title': 'min', 'category': 'min'})\n",
    "df_val_grouped = df_val.groupby(['id']).agg({'attributes': 'sum', 'title': 'min', 'category': 'min'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e2a352f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\ADMIN\\Miniconda3\\envs\\piechatgpt\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = 'microsoft/deberta-v3-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "399d3b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B-Flash_Memory_Cards_Capacity': 1, 'I-Flash_Memory_Cards_Capacity': 2, 'B-Digital_Cameras_Sensor_Size': 3, 'I-Digital_Cameras_Sensor_Size': 4, 'B-Flash_Memory_Cards_Memory_Stick_Format': 5, 'I-Flash_Memory_Cards_Memory_Stick_Format': 6, 'B-Laptops_Screen_Size': 7, 'I-Laptops_Screen_Size': 8, 'B-Digital_Cameras_Resolution': 9, 'I-Digital_Cameras_Resolution': 10, 'B-Laptops_Processor_Speed': 11, 'I-Laptops_Processor_Speed': 12, 'B-Laptops_Resolution': 13, 'I-Laptops_Resolution': 14, 'B-Laptops_Battery_Life': 15, 'I-Laptops_Battery_Life': 16, 'B-Laptops_Weight': 17, 'I-Laptops_Weight': 18, 'B-Laptops_Number_of_Cores': 19, 'I-Laptops_Number_of_Cores': 20, 'B-Digital_Cameras_Optical_Zoom': 21, 'I-Digital_Cameras_Optical_Zoom': 22, 'B-Flash_Memory_Cards_SD_Format': 23, 'I-Flash_Memory_Cards_SD_Format': 24, 'B-Laptops_Refresh_Rate': 25, 'I-Laptops_Refresh_Rate': 26, 'B-Laptops_Processor_Brand': 27, 'I-Laptops_Processor_Brand': 28, 'B-Digital_Cameras_Sensor_Type': 29, 'I-Digital_Cameras_Sensor_Type': 30, 'B-Digital_Cameras_Camera_Weight': 31, 'I-Digital_Cameras_Camera_Weight': 32, 'B-Digital_Cameras_Type': 33, 'I-Digital_Cameras_Type': 34, 'B-Flash_Memory_Cards_Format': 35, 'I-Flash_Memory_Cards_Format': 36}\n"
     ]
    }
   ],
   "source": [
    "# Determine NER tags relevant - Focus on ner tags contained in the test set\n",
    "def extract_ner_tags(example):\n",
    "    new_ner_tags = ['{}_{}'.format('_'.join(example['category'].split(' ')), '_'.join(attribute['key'].split(' '))) for attribute in example['attributes']]\n",
    "    return new_ner_tags\n",
    "\n",
    "ner_tags = set()\n",
    "\n",
    "[ner_tags.update(tags) for tags in df_train_grouped.apply(extract_ner_tags, axis=1).tolist()]\n",
    "\n",
    "# Create processed ner tags\n",
    "processed_ner_tags = ['O']\n",
    "for ner_tag in ner_tags:\n",
    "    processed_ner_tags.append('B-{}'.format(ner_tag))\n",
    "    processed_ner_tags.append('I-{}'.format(ner_tag))\n",
    "\n",
    "# Create dict ner_tags 2 numbers in list\n",
    "processed_ner_tags_2_number = dict(zip(processed_ner_tags, [i for i in range(0, len(processed_ner_tags))]))\n",
    "print(processed_ner_tags_2_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bd5dfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_ner_tags(example):\n",
    "    token_input = tokenizer(example['title'])\n",
    "    example['tokens'] = tokenizer.convert_ids_to_tokens(token_input['input_ids'])\n",
    "    \n",
    "    ner_tags = [0 for token in example['tokens']]\n",
    "    for attribute in example['attributes']:\n",
    "        cat_attr = '{}_{}'.format('_'.join(example['category'].split(' ')), '_'.join(attribute['key'].split(' ')))\n",
    "        if 'B-{}'.format(cat_attr) in processed_ner_tags:\n",
    "            for evidence in attribute['evidences']:\n",
    "                if evidence['value'] in example['title']:\n",
    "                    begin = evidence['begin'] + len(example['tokens'][0]) + 1 #Take care of CLS token\n",
    "                    end = evidence['end'] + len(example['tokens'][0]) + 1\n",
    "                    token_position = 0\n",
    "                    found_beginning = False\n",
    "                    position_update = {}\n",
    "                    relevant_tokens = example['tokens'].copy()\n",
    "                    current_token = relevant_tokens[0]\n",
    "                    relevant_tokens = relevant_tokens [1:]\n",
    "                    for position in range(0, len(example['title'])):\n",
    "                        if not found_beginning:\n",
    "                            if position == begin:\n",
    "                                position_update[token_position] = 'B-{}'.format(cat_attr)\n",
    "                                found_beginning = True \n",
    "                            \n",
    "                        elif position >= begin and position < end and token_position not in position_update:\n",
    "                            position_update[token_position] = 'I-{}'.format(cat_attr)\n",
    "                        \n",
    "                        if position > end  + 1:\n",
    "                            break\n",
    "                        \n",
    "                        # Shorten current token\n",
    "                        if len(current_token) > 1:\n",
    "                            current_token = current_token[1:]\n",
    "                        \n",
    "                        # Jump to next token\n",
    "                        elif len(current_token) == 1:\n",
    "                            current_token = relevant_tokens[0]\n",
    "                            relevant_tokens = relevant_tokens [1:]\n",
    "                            token_position += 1\n",
    "                        else:\n",
    "                            print(current_token)\n",
    "                            print('Something went wrong!')\n",
    "                    \n",
    "                    # Assign positions\n",
    "                    for position, found_ner_tag in position_update.items():\n",
    "                            ner_tags[position] = processed_ner_tags_2_number[found_ner_tag]\n",
    "    ner_tags[0] = -100\n",
    "    ner_tags[-1] = -100\n",
    "    return ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc4a0921",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_grouped['ner_tags'] = df_train_grouped.apply(assign_ner_tags, axis=1)\n",
    "df_val_grouped['ner_tags'] = df_val_grouped.apply(assign_ner_tags, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11a208eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>category</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1060220822</th>\n",
       "      <td>Essential 64GB Samsung Galaxy Tab 10.1 Micro S...</td>\n",
       "      <td>Flash Memory Cards</td>\n",
       "      <td>[-100, 0, 11, 12, 0, 0, 0, 0, 0, 0, 29, 17, 30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106022206X</th>\n",
       "      <td>Essential 64GB AT&amp;T F160 Micro SDHC Card is cu...</td>\n",
       "      <td>Flash Memory Cards</td>\n",
       "      <td>[-100, 0, 11, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060223929</th>\n",
       "      <td>Essential 64GB HTC EVO 4G LTE Micro SDHC Card ...</td>\n",
       "      <td>Flash Memory Cards</td>\n",
       "      <td>[-100, 0, 11, 12, 0, 0, 0, 0, 0, 29, 17, 30, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106023565X</th>\n",
       "      <td>Essential 64GB Sony Xperia miro Micro SDHC Car...</td>\n",
       "      <td>Flash Memory Cards</td>\n",
       "      <td>[-100, 0, 11, 12, 0, 0, 0, 0, 29, 30, 30, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1060237482</th>\n",
       "      <td>Essential 64GB Acer Iconia A1-830 Micro SDHC C...</td>\n",
       "      <td>Flash Memory Cards</td>\n",
       "      <td>[-100, 0, 11, 12, 0, 0, 0, 0, 0, 0, 0, 29, 17,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B01HHCJEK0</th>\n",
       "      <td>Canon EOS 6D Digital SLR Camera with EF 24-105...</td>\n",
       "      <td>Digital Cameras</td>\n",
       "      <td>[-100, 0, 13, 0, 0, 13, 14, 14, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B01HHR572O</th>\n",
       "      <td>Sigma SD Quattro Digital Camera with 30mm F1.4...</td>\n",
       "      <td>Digital Cameras</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B01HI9NT6M</th>\n",
       "      <td>2019 Samsung 11.6” Thin &amp; Lightweight HD Chrom...</td>\n",
       "      <td>Laptops</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B01HIJIZR0</th>\n",
       "      <td>2016 High Performance Flagship Toshiba FHD IPS...</td>\n",
       "      <td>Laptops</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 33, 34, 34, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B01HIPPXBU</th>\n",
       "      <td>HP 15AY075NR / 15-AY075NR / X0H79UA#ABA 15, In...</td>\n",
       "      <td>Laptops</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18083 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        title  \\\n",
       "id                                                              \n",
       "1060220822  Essential 64GB Samsung Galaxy Tab 10.1 Micro S...   \n",
       "106022206X  Essential 64GB AT&T F160 Micro SDHC Card is cu...   \n",
       "1060223929  Essential 64GB HTC EVO 4G LTE Micro SDHC Card ...   \n",
       "106023565X  Essential 64GB Sony Xperia miro Micro SDHC Car...   \n",
       "1060237482  Essential 64GB Acer Iconia A1-830 Micro SDHC C...   \n",
       "...                                                       ...   \n",
       "B01HHCJEK0  Canon EOS 6D Digital SLR Camera with EF 24-105...   \n",
       "B01HHR572O  Sigma SD Quattro Digital Camera with 30mm F1.4...   \n",
       "B01HI9NT6M  2019 Samsung 11.6” Thin & Lightweight HD Chrom...   \n",
       "B01HIJIZR0  2016 High Performance Flagship Toshiba FHD IPS...   \n",
       "B01HIPPXBU  HP 15AY075NR / 15-AY075NR / X0H79UA#ABA 15, In...   \n",
       "\n",
       "                      category  \\\n",
       "id                               \n",
       "1060220822  Flash Memory Cards   \n",
       "106022206X  Flash Memory Cards   \n",
       "1060223929  Flash Memory Cards   \n",
       "106023565X  Flash Memory Cards   \n",
       "1060237482  Flash Memory Cards   \n",
       "...                        ...   \n",
       "B01HHCJEK0     Digital Cameras   \n",
       "B01HHR572O     Digital Cameras   \n",
       "B01HI9NT6M             Laptops   \n",
       "B01HIJIZR0             Laptops   \n",
       "B01HIPPXBU             Laptops   \n",
       "\n",
       "                                                     ner_tags  \n",
       "id                                                             \n",
       "1060220822  [-100, 0, 11, 12, 0, 0, 0, 0, 0, 0, 29, 17, 30...  \n",
       "106022206X  [-100, 0, 11, 12, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "1060223929  [-100, 0, 11, 12, 0, 0, 0, 0, 0, 29, 17, 30, 0...  \n",
       "106023565X  [-100, 0, 11, 12, 0, 0, 0, 0, 29, 30, 30, 0, 0...  \n",
       "1060237482  [-100, 0, 11, 12, 0, 0, 0, 0, 0, 0, 0, 29, 17,...  \n",
       "...                                                       ...  \n",
       "B01HHCJEK0  [-100, 0, 13, 0, 0, 13, 14, 14, 0, 0, 0, 0, 0,...  \n",
       "B01HHR572O  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "B01HI9NT6M  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "B01HIJIZR0  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 33, 34, 34, ...  \n",
       "B01HIPPXBU  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[18083 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_grouped_reduced = df_train_grouped[['title', 'category','ner_tags']]\n",
    "df_val_grouped_reduced = df_val_grouped[['title', 'category','ner_tags']]\n",
    "df_train_grouped_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f046d5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets\n",
    "# Convert to huggingface dataset\n",
    "from datasets import Dataset, DatasetDict, ClassLabel, Features, Value, Sequence\n",
    "\n",
    "features = Features({'title': Value('string'), \n",
    "                     'category' : Value('string'), \n",
    "                     'ner_tags': Sequence(feature=ClassLabel(names=processed_ner_tags)), \n",
    "                     'id': Value('string')})\n",
    "\n",
    "train_dataset = Dataset.from_pandas(df_train_grouped_reduced, features=features)\n",
    "val_dataset = Dataset.from_pandas(df_val_grouped_reduced, features=features)\n",
    "\n",
    "raw_datasets = DatasetDict({\"train\":train_dataset, \"val\": val_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4192c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_sequences(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"title\"])\n",
    "    tokenized_inputs[\"labels\"] = examples[\"ner_tags\"]\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c59b496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "643f0a38d03c4d6d8cc5d3d4190e6c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/18083 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98501643202d433e83e4b529d1b168b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2212 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(\n",
    "    tokenize_and_align_sequences,\n",
    "    batched=True,\n",
    "    remove_columns=raw_datasets[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee56658b",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d797157a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1bf34e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-100,    0,   27,   28,    0,    0,    0,    0,    0,    0,   15,   19,\n",
       "           16,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0, -100],\n",
       "        [-100,    0,   27,   28,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "         -100, -100]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "batch[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b57cbe3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install seqeval\n",
    "#!pip install evaluate\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa4c9f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54e16934",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\n",
    "label_names = ner_feature.feature.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b4fe6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(label_names)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4409c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2ForTokenClassification: ['lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.bias', 'mask_predictions.classifier.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.weight', 'deberta.embeddings.position_embeddings.weight']\n",
      "- This IS expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForTokenClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7fd49fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]=\"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "39f1e26f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip3 install torch torchvision torchaudio\n",
    "#!pip install accelerate -U\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/ceph/alebrink/MAVE/baselines/named_entity_recognition/deberta-v3-large-finetuned-ner-10epochs-V2\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_f1',\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494af402",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"val\"],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    \n",
    ")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
